---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: "Academic Log | June/July 2022"
subtitle: ""
summary: ""
authors: ["Shreyansh Singh"]
tags: [reading-list, nlp, annotated paper, gpu, deep learning]
categories: [Machine Learning, Computer Science]
date: 2022-08-04T00:27:33+05:30
lastmod: 2022-08-04T00:27:33+05:30
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: ""
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
---

A collection of academic papers/blogs/talks/projects that I read/watched/explored during the month. I also include any small (or large) personal projects that I did and any such related ML/non-ML work.

## Personal Projects

- **Paper re-implementation** - "Extracting Training Data from Large Language Models" by Carlini et al., 2021. - [[Github]](https://github.com/shreyansh26/Extracting-Training-Data-from-Large-Langauge-Models)

## Annotated Papers

- [Learning Backward Compatible Embeddings](https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/General-DL/Learning%20Backward%20Compatible%20Embeddings.pdf)
- [Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models](https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/LLMs/Memorization%20Without%20Overfitting%20-%20Analyzing%20the%20Training%20Dynamics%20of%20Large%20Language%20Models.pdf)
- [Tracing Knowledge in Language Models Back to the Training Data](https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/LLMs/Tracing%20Knowledge%20in%20Language%20Models%20Back%20to%20the%20Training%20Data.pdf)

## Papers I read

- [On the Unreasonable Effectiveness of Feature propagation in Learning on Graphs with Missing Node Features](https://arxiv.org/abs/2111.12128)
- [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311)
- [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://cdn.openai.com/papers/dall-e-2.pdf)
- [Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language](https://arxiv.org/abs/2204.00598)
- [Unified Contrastive Learning in Image-Text-Label Space](https://arxiv.org/abs/2204.03610v1)
- [Improving Passage Retrieval with Zero-Shot Question Generation](https://arxiv.org/abs/2204.07496)
- [Exploring Dual Encoder Architectures for Question Answering](https://arxiv.org/abs/2204.07120)
- [Efficient Fine-Tuning of BERT Models on the Edge](https://arxiv.org/abs/2205.01541)
- [Fine-Tuning Transformers: Vocabulary Transfer](https://arxiv.org/abs/2112.14569)
- [Manipulating SGD with Data Ordering Attacks](https://arxiv.org/abs/2104.09667)
- [Differentially Private Fine-tuning of Language Models](https://openreview.net/forum?id=Q42f0dfjECO)
- [Extracting Training Data from Large Language Models](https://arxiv.org/abs/2012.07805)
- [Learning Backward Compatible Embeddings](https://arxiv.org/abs/2206.03040)
- [Compacter: Efficient Low-Rank Hypercomplex Adapter Layers](https://arxiv.org/abs/2106.04647)
- [Agreement-on-the-Line: Predicting the Performance of Neural Networks under Distribution Shift](https://arxiv.org/abs/2206.13089)
- [Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models](https://arxiv.org/abs/2205.10770)
- [Tracing Knowledge in Language Models Back to the Training Data](https://arxiv.org/abs/2205.11482)

## Blogs I read

- [Domain Adaptation with Generative Pseudo-Labeling (GPL)](https://www.pinecone.io/learn/gpl/)
- [Making Deep Learning Go Brrrr From First Principles](https://horace.io/brrr_intro.html)
- [Introduction to TorchScript](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html)
- [Nonlinear Computation in Deep Linear Networks](https://openai.com/blog/nonlinear-computation-in-linear-networks/)

## Talks I watched

- [How GPU Computing Works](https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31151/)